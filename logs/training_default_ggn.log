2024-11-17 18:20:43
labels: {'easy': 0, 'medium': 1, 'hard': 2}
[DLOG]  ------------ Args Saved! -------------
[DLOG]  args is : by DLOG: Namespace(seed=1992, server_tag='seizure', out_middle_features=False, debug=False, task='ggn', dataset='MANTIS', data_path='/home/ebocini/repos/mantis_data/ggn_data', adj_file='/home/ebocini/repos/GGN-update/adjs/raw_adj.npy', adj_type='er', testing=False, arg_file='None', independent=False, using_fc=False, unit_test=False, multi_train=False, focalloss=False, focal_gamma=2.0, weighted_ce='prop', dev=False, dev_size=1000, best_model_save_path='/home/ebocini/repos/GGN-update/best_models/training_default_ggn.pth', pre_model_path='./best_models/seed_pretrain_08021405', batch_size=32, epochs=100, lr=0.0005, lr_decay_rate=0.92, weight_decay=0.0001, dropout=0.3, clip=3, seq_length=3, predict_len=12, scheduler=False, mo=0.1, cuda=True, transpose=False, runs=1, fig_filename='/home/ebocini/repos/GGN-update/figs/training_default_ggn', not_using_gnn=False, gnn_name='gwn', gnn_pooling='gate', agg_type='gate', gnn_layer_num=2, gnn_hid_dim=32, gnn_out_dim=16, gnn_fin_fout='1100,550;550,128;128,128', gnn_res=False, gnn_adj_type='None', gnn_downsample_dim=0, coarsen_switch=3, using_cnn=False, gate_t=False, att=False, recur=False, fusion=False, pretrain=False, feature_len=126, gwn_out_features=32, wavelets_num=16, rnn_layer_num=2, rnn_in_channel=32, rnn=False, bidirect=True, gcn_out_features=32, rnn_hidden_len=32, max_diffusion_step=2, eeg_seq_len=250, predict_class_num=3, encoder='rnn', encoder_hid_dim=256, cut_encoder_dim=0, decoder='lgg_cnn', decoder_type='conv2d', decoder_downsample=-1, decoder_hid_dim=512, decoder_out_dim=32, predictor_num=3, predictor_hid_dim=512, em_train=True, lgg=True, lgg_time=False, lgg_warmup=5, lgg_tau=0.01, lgg_hid_dim=64, lgg_k=5, dcgru_activation='tanh')
load seizure data, shape: (1307, 40, 16, 126) (1307,)
WARNING: shuffling takes a long time, skip it at the moment, add it later !!!!!!!
before trans: (1091, 40, 16, 126) (1091,) (216, 40, 16, 126) (216,)
after trans: (1091, 126, 16, 40) (1091,) (216, 126, 16, 40) (216,)
2
(1091, 126, 16, 40)
num_batch  35
/home/ebocini/repos/GGN-update/eeg_util.py:205: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)
  xs = torch.Tensor(xs)
num_batch  7
num_batch  7
tensorboard path: ./tfboard/seizure/11_17_18_20
WARNING: creating adjecency matrix using ER! Using 16 channels, change this for the actual run!!!!
N: 16
adj_fix torch.Size([16, 16])
[DLOG]  Predictor in channel: 48
[DLOG]  -------- ecoder: -----------
 RNNEncoder(
  (rnn): GRU(126, 256, num_layers=2, batch_first=True, bidirectional=True)
)
[DLOG]  -------- decoder: -----------
 SpatialDecoder(
  (gnn_decoder): GNNDecoder(
    (gnns): ModuleList(
      (0): GraphConv(
        (lin): Linear(in_features=512, out_features=32, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (1): GraphConv(
        (lin): Linear(in_features=32, out_features=16, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (g_pooling): GateGraphPooling()
  )
  (cnn_decoder): CNN2d(
    (b1): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Dropout(p=0.3, inplace=False)
    )
    (bx): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Dropout(p=0.3, inplace=False)
      )
    )
    (bn): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
      (3): ReLU()
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Dropout(p=0.3, inplace=False)
    )
  )
)
args_cuda: True
rnn_train RNNBlock to cuda!
weights: tensor([0.6542, 0.6565, 0.6894], device='cuda:0')
epoch: 0
train_loss          1.105366
train_acc     tensor(0.3245)
val_loss            1.105181
val_acc       tensor(0.3472)
dtype: object
update best model, epoch:  0
train_loss          1.105366
train_acc     tensor(0.3245)
val_loss            1.105181
val_acc       tensor(0.3472)
dtype: object
update best model, epoch:  1
train_loss          1.099613
train_acc     tensor(0.3767)
val_loss            1.102804
val_acc       tensor(0.3611)
dtype: object
update best model, epoch:  4
train_loss          0.940865
train_acc     tensor(0.5573)
val_loss            1.192086
val_acc       tensor(0.4120)
dtype: object
update best model, epoch:  5
train_loss          0.830863
train_acc     tensor(0.6425)
val_loss            1.280274
val_acc       tensor(0.4167)
dtype: object
epoch: 20
train_loss          0.113314
train_acc     tensor(0.9679)
val_loss            2.937944
val_acc       tensor(0.3333)
dtype: object
epoch: 40
train_loss          0.007916
train_acc     tensor(0.9973)
val_loss             3.85784
val_acc       tensor(0.3704)
dtype: object
epoch: 60
train_loss           0.00381
train_acc     tensor(0.9991)
val_loss            4.401567
val_acc       tensor(0.3333)
dtype: object
epoch: 80
train_loss          0.002002
train_acc         tensor(1.)
val_loss            4.355467
val_acc       tensor(0.3380)
dtype: object
training: :
after training adj_fix tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
       device='cuda:0', grad_fn=<SelectBackward0>)
best_epoch: 5
N: 16
adj_fix torch.Size([16, 16])
[DLOG]  Predictor in channel: 48
[DLOG]  -------- ecoder: -----------
 RNNEncoder(
  (rnn): GRU(126, 256, num_layers=2, batch_first=True, bidirectional=True)
)
[DLOG]  -------- decoder: -----------
 SpatialDecoder(
  (gnn_decoder): GNNDecoder(
    (gnns): ModuleList(
      (0): GraphConv(
        (lin): Linear(in_features=512, out_features=32, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (1): GraphConv(
        (lin): Linear(in_features=32, out_features=16, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (g_pooling): GateGraphPooling()
  )
  (cnn_decoder): CNN2d(
    (b1): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Dropout(p=0.3, inplace=False)
    )
    (bx): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Dropout(p=0.3, inplace=False)
      )
    )
    (bn): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
      (3): ReLU()
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Dropout(p=0.3, inplace=False)
    )
  )
)
/home/ebocini/repos/GGN-update/eeg_main.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_model.load_state_dict(torch.load(model_save_path), strict=False)
after load best model adj_fix tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
       device='cuda:0', grad_fn=<SelectBackward0>)
test:
test_acc     tensor(0.4167)
test_loss           1.28029
dtype: object
['ea', 'medi', 'ha']
Confusion: [[0.51 0.27 0.23]
 [0.42 0.34 0.24]
 [0.25 0.34 0.4 ]]
micro f1: 0.4166666666666667, macro f1: 0.41425567222004084, weighted f1: 0.4143822979821784
finish rnn_train!, time cost: 180.3782253265381
type:basic model20241117,trials: 1, t loss mean/std: 1.280290/0.000000, t acc mean/std: 0.416667%/0.000000 

Main running Over, total time spent: 183.3290078639984
2024-11-17 18:23:46
